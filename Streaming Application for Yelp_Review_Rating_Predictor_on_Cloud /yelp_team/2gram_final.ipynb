{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2gram-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlypCWMQGoWt"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import DecisionTreeClassifier,NaiveBayes,LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import NGram,OneHotEncoder, StringIndexer, VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw1B6iSLGtny"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_VdY_ppGwpo"
      },
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"text_classification_trainer\") \\\n",
        "    .master(\"local\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwwEPK51GxJS"
      },
      "source": [
        "df = spark.read.json(\"gs://bdl2021_final_project/yelp_train.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekCU6PzwGxrx"
      },
      "source": [
        "data = df.select('text', 'stars').dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqdFBvsNG2YW"
      },
      "source": [
        "data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvpW3ZppG2uw"
      },
      "source": [
        "def build_twograms(inputCol=[\"text\",\"stars\"], n=2):\n",
        "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
        "    \n",
        "    ngrams = [\n",
        "    \n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        " \n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=1024,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
        " \n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features\"\n",
        "    )]\n",
        "     \n",
        "    lr = [LogisticRegression(labelCol=\"stars\", featuresCol=\"features\",maxIter=20, regParam=0.1)]  \n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler +lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rcpAmZ0G7eV"
      },
      "source": [
        "# print(\"doing..\")\n",
        "# twogram_pipelineFit = build_twograms().fit(data)\n",
        "# print(\"doing..\")\n",
        "# twogram_pipelineFit.save('gs://murari/modelpath/')\n",
        "# print(\"done!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb0HjR0fHF3v"
      },
      "source": [
        "model = PipelineModel.load('gs://murari/modelpath/') #Load model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-gJ4yeDHBHw"
      },
      "source": [
        "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"stars\", predictionCol=\"prediction\")\n",
        "predictionAndTarget = model.transform(data).select(\"stars\", \"prediction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypoEP_dyHB9G"
      },
      "source": [
        "print(\"doing...\")\n",
        "acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
        "print(\"Accuracy of logistic is = %g\"% (acc))\n",
        "print(\"F1 of logistic is = %g\"% (f1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}